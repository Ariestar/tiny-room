---
tags:
  - 数学
  - math
  - 读后感
  - 读书笔记
  - 美
  - 计算机
  - 信息
status: publish
date created: 2024-07-18 09:52:50
date modified: 2025-05-01 12:55:17
author:
  - 吴军
---
通信问题概率模型  
$$  
s_{1},s_{2},s_{3},\dots=\underbrace{ ArgMax }_{ all s_{1},s_{2},s_{3},\dots }\frac{P(o_{1},o_{1},o_{3},\dots|s_{1},s_{2},s_{3}\dots)·P(s_{1},s_{2},s_{3}\dots)}{P(o_{1},o_{2},o_{3},\dots)}  
$$

# 隐含马尔科夫模型 

对于一个随机过程，假设各个状态 $St$ 的概率分布，只与其前一状态 $S_{t-1}$ 有关，这个假设被成为马尔可夫假设，这个随机过程成为马尔科夫链/马尔可夫过程

## 隐含马尔可夫模型  

==独立输出假设==：每个时刻 t 输出一个符号 Ot，Ot 仅与 St 相关  
满足独立输出假设的马尔可夫模型称为==“隐含”马尔可夫模型==

隐含马尔可夫模型恰好满足通信解码问题，只需代入马尔科夫假设和独立输出假设

得到 $$  
P(o_{1},o_{2},o_{3}|s_{1},s_{2},s_{3}\dots)=\prod_{t}P(s_{t}|s_{t-1})·P(o_{t}|s_{t})  
$$  
维特比（Viterbi）算法

对不同应用，$P(o_{1},o_{2},o_{3}|s_{1},s_{2},s_{3}\dots)$ 名称不同  
语音识别：声学模型  
机器翻译：翻译模型  
拼写矫正：纠错模型

李开复：世界上第一个大词汇量连续语音识别系统 Sphinx

# 信息的度量

## 信息熵

香农（Claude Shannon）论文“通信的数学原理”中提到信息熵

对于任一随机变量 $X$，其信息熵定义为 $$  
H(X)=-\sum_{x\in X}P(x)logP(x)  
$$  
信息量与不确定性直接相关，==不确定性越大，信息熵越大==  
“熵”源自热力学概念，两者同为==系统无序性的度量==

一本 50 万字的中文书，信息量大约为 320KB，若用国际编码存储，大约需要 1MB  
标准信息量与实际信息量之间的差值为==冗余度==（reduncancy）  
汉语冗余度很小

## 信息的作用

==信息用来消除不确定性==，打开黑盒子

## 条件熵

利用联合概率分布（Joint Probability）可以求出条件概率分布（Conditional Probability）  
$$  
H(X|Y)=-\sum_{x\in X}P(x,y)\log P(X|Y)  
$$

$H(X)\geq H(X|Y)$  
即知道更多 Y 的信息后，X 的熵减少  
因此二元模型不确定性小于一元模型

## 互信息

==量化多个随机变量间相关性==

互信息定义 $$  
I(X;Y):=H(X)-X(X|Y)=\sum_{x\in X,y\in Y}P(x,y)\log \frac{P(x,y)}{P(x)P(y)}  
$$

## 相对熵/交叉熵

计算==两个取值为正的函数间相似性==，由库贝尔克和莱博勒提出

$$  
KL(f(x)||g(x))=\sum_{x\in X}f(x)\log \frac{f(x)}{g(x)}  
$$
1. 两个相同函数，相对熵等于零
2. 与两个函数的差异正相关

由于交叉熵没有交换律，因此香农和詹森提出平均形式的交叉熵  
$$  
JS(f(x)||g(x))=\frac{1}{2}[KL(f(x)||g(x))+KL(g(x)||f(x))]  
$$  
应用
1. 两个常用词在不同文本中概率分布，看是否同义
2. 推出词频率 - 逆向文档频率（TF-IDF），这是信息检索中最重要的概念之一

