---
tags:
  - ML
  - 机器学习
  - 深度学习
  - 神经网络
  - AI
status: publish
date created: 2024-07-18 09:52:53
date modified: 2025-08-28 18:52:24
---

# 机器学习基本流程

Machine Learning 在干什么：寻找一个**function**

Machine Learning 的 traning
- 写出带有未知量的 model function
- 定义 loss function
- optimization：逼近需要的函数

线性函数只能是 linear 的，无法逼近复杂的函数，所以存在较大的 **model bias**，但是如果使用**分段线性函数**，并且分段足够多，就能逼近任何一个函数

这种类线性函数，统称为 activation function，我们可以使用 sigmoid 函数，或者 ReLU 函数  
通过改变参数的值，可以调整 activation function 的伸缩变化
$$
sigmoid=\frac{1}{1+e^{-(b+wx)}}
$$
$$ReLU=max\left( 0,b+wx \right)$$

我们将特征进行加权求和之后，经过多个激活函数得到的结果进行加权求和，得到新的预测值

因此，我们定义新的模型函数
$$
y=b+\sum_{i}\left (c_{i}+sigmoid\left( b_{i}+\sum_{j}w_{ij}x_{j} \right)\right)
$$

我们可以用矩阵和向量来**简写上述模型**
$$
y=b+c^{T}\sigma(\mathbf{b}+\mathbf{W}\mathbf{x})
$$

将需要训练的参数向量展开放在一个新的向量 $\boldsymbol{\theta}$ 中，我们新的优化方式就变成了求 $\boldsymbol{\theta}^{*}=arg\min_{\boldsymbol{\theta}}L$

在实际 traing 中，我们会将 N 维的 feature 随机分成若干 batch，每次 Batch 进行一次 $\boldsymbol{\theta}$ 的 update，所有 batch 都进行了一次叫做一个 epoch

参数的训练方法，叫做 gradient descent 梯度下降

# 训练过程问题

## 学习率

learning rate 也是一个需要调整的超参数，如果 learning rate 太小，在平缓的地方就会下降很慢，如果 learning rate 太大，很容易走的太快没法收敛

所以，我们希望学习率可以自动调整，将 learning rate $\eta$ 改为 $\frac{\eta}{\sigma_{i}^{t}}$，这个整体就 parameter independent

$$
\theta_{i}^{t+1}<-\theta _{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}
$$



---

# Terminology