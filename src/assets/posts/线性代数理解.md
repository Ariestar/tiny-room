---
tags:
  - 线性代数
  - 高等代数
  - 矩阵
  - 数学
status: publish
date created: 2024-07-18 09:52:54
date modified: 2025-08-27 18:28:31
---

> Abstract is the price of generality.

满足下面八条公理，可以称为 vec. space 
- 结合律

向量空间本身可以很抽象，它的元素并非一定为 scalar，也可以是 x、物理符号等等。一个向量空间用它的 basis 来描述，它的维度就是 basis 的元素个数。

线性代数将 field(域) 这个结构作为研究对象，因为 field 对

如果把向量空间中的元素用 basis vector 的系数表示，就成为坐标向量。坐标向量中的值代表每个 basis vector 的份数/个数。这样，我们把抽象的元素表示成简单的数值。

线性变换本质上是一种线性的 map，映射两个 l. space，这要求它对加法和数乘封闭。

考虑坐标向量，把新的线性空间的 basis 放在一起，就构成了矩阵。一个矩阵左乘任何向量，即把该向量映射到新的线性空间中。两个矩阵相乘得到的新的矩阵，就是把两个线性变换的效果组合，本质上就是函数的简单 composition $f \circ g$，矩阵乘法由此定义。 线性变换的逆过程（如果存在），代表的是逆矩阵。

先有抽象的 l. trans.，然后通过选择选择 basis，就可以将 l. trans.转化成 matrix。所以，matrix 可以理解成 l. trans. 在选取的 basis 下的具象化表示

l. trans. 不能增加 l. space 的 dim，但有可能降低 dim。变换出的 vec. space 的 dim，定义为 rank。矩阵的 rank 等于其 column vector 进行 l. comb. 张成的 column space 的 dim，也就是新的 basis 所能张成的 space 的 dim

dim 降低意味着原线性空间中的一些向量会被压缩成零向量。这些向量张成一个子空间，定义为 Null Space/Kernel。

one-to-one(injective, 单射) 指的是除了零向量，没有其他向量通过 l. trans.能变成 0  
$\iff$ $kernal=\{ 0 \}$  
$\iff$ 输入空间的 dim 不下降，输入空间也就是 column space

onto(surjective, 满射) 指的是映射过去的值域能充满整个空间  
$\iff$ 

matrix invertable $\iff$ one-to-one(injective, 单射)$\iff$ $\iff$ onto(surjective, 满射)$\iff$ 



维度降低也意味着行列式=0。行列式几何意义上代表的是新空间中的有向面积。有向面积为 0 也就是空间被压缩了。

基向量的选择是随意的。不同的选择可以通过线性变换去相互转化，这样的线性变化对应的矩阵称为坐标矩阵。有时改变坐标系能简化运算。

线性方程组，可以用系数矩阵×自变量向量的形式表示。方程，其实蕴含着“==逆向找==”的意味。用线性变换理解，也就是找变换之前的那个向量的过程。不改变线性方程组解的操作对应的矩阵定义为基础矩阵。高斯消元就是通过基础操作找到系数矩阵的 rank。如果维度不降低（$dim(R(M))=\#variables$），那么就可以用逆矩阵找到原来的向量。如果维度降低，矩阵不可逆，只有当要求的向量刚好在低维空间中时，可以求解，否则方程组无解。

有一些向量，在线性变化过程中方向不变，只发生 scale，这样的向量定义为 特征向量，张成的空间定义为 特征空间， scale 的倍数定义为 特征值。对一个线性变换，特征值可能有多个。

# 可对角化  

当特征空间的个数刚好等于向量空间的维度时候，这些特征空间的 basis 刚好可以作为该空间的 basis（基变换）。由于只发生 scale，这个线性变换对应的矩阵是一个对角矩阵。

一个方阵可对角化等价于 
- （基）可以找到一组由 eigenvector 组成的一组基
- （subspace）可以找到 $\dim V$ 个 T-invariant 的 1 维 subspace（PS：T-invariant 的矩阵是对角块矩阵）
- （eigenspace）可以把 T 分解成 m 个 eigenspace 的值和（$m\leq \dim V$）
- （维度）eigenspace 的维度之和=$\dim V$  
- （重数）每个重数 $k_{i}=\dim E_{\lambda _{i}}$  
- 𝑀的==极小多项式==经标准分解后，每一项都是一次项，且重数都是 1。（称作互异单根条件）  

对角化，直观上把矩阵变成对角矩阵。  
从列向量的角度，这就是找到了 n 组线性独立的 vector，这些 vector 只在==与对角线相交的地方有值 (eigenvalue)==，这意味着只在这个方向上拉伸，也就是 $\lambda v=Tv$；这些列向量是 1-dim 且 T-invariant 的。  
把不同的 eigenvalue 分开成 m 组，这 m 组就可以构成 m 个 eigenspace，他们的维度之和当然 $=\dim V$，为什么相同的 eigenvalue 对应的 eigenvector 可以构成一组 eigenspace 的基呢？因为 eigenspace $E_{\lambda}:=\{ x\in V|Tx=\lambda x \}=\{ x\in V|(T-\lambda I)x=0 \}=N(T-\lambda I)$，相同的 eigenvalue 只是说拉伸的比例相同，但是可以是不同方向的拉伸。（比如在 $\mathbb{R}^{3}$ 中，eigenspace 可以是一个两倍拉伸的一个 $\mathbb{R}^{2}$ 平面）  
从重数的角度，定义 multiplicity $m:=\dim G(\lambda,T)$。由于任何一个 $T\in \mathcal{L}(V)$，不管是否可对角化， 都可以分解成 generalized eigenspace 的值和，也就是都可以满足重数之和=$\dim V$，那么只要我对每个 eigenspace，求他的 dim，都等于对应的重数，那么 eigenspace 的维度之和=重数之和=V 的维度，那么矩阵可对角化

那么矩阵不可对角说明什么？说明找不到一组 eigenvector 构成的基，eigenspace 的值和也并不能覆盖整个 V ($\sum \dim E_{\lambda}<dim V$)。

## 广义 eigenspace

对于不能对角化的矩阵，他们 $E_{\lambda}$ 的维度和不足以找到一组 ==基的并== 作为 V 的基；我们可以退而求其次，广义 eigenspace 就解决了这个问题，从而让所有方阵都可以变为 Jordan Form，它类似对角矩阵，只是可能在对角线上一排存在 1

广义 eigenspace $K_{\lambda}:=\{ x|(T-\lambda I)^n(x)=0 \}=N((T-\lambda I)^n)$  
$K_{\lambda}$ 有几个很好的性质
- V 可以分解成有限个 $K_{\lambda_{i}}$ 的值和
- $T|_{K_{\lambda_{i}}}$ 是 T-invariant 的
- $T-\lambda_{i}I|_{K_{\lambda_{i}}}$ 是一个 nilpotent

而对一个 nilpotent，我们可以找到一组由 disjoint union of cycles 组成的一组基，让他的矩阵是一个块对角矩阵，每个 block 有如下形式
$$
\begin{bmatrix}
0 & 1 &  & 0 \\
 & \ddots & \ddots &  \\
 &  & \ddots & 1 \\
0 &  &  & 0 \\
\end{bmatrix}
$$

而对每一个 eigenvalue，他的 $T-\lambda I|_{K_{\lambda}}$ 是一个 nilpotent，所以可找到一组基，让 $T-\lambda I$ 有上面的矩阵形式，所以 $T$ 就是在==对角线上加上了 $\lambda$==，就形成了 一个 $K_{\lambda}$ 的 Jordan basis。把所有 $K_{\lambda}$ 的 Jordan basis 的基==合起来==，整体又是一个 ==T== 的 Jordan basis

由广义 eigenspace 的定义，$T-\lambda I|_{G_{\lambda}}$ 是一个 nilpotent，因为 $(T-\lambda I)^m(v)=0$。  
$(T-\lambda I)^{\dim G}=0$，$\{ (T-\lambda I)^{m_{1}}(v_{1}),\dots , (T-\lambda I)^{m_{n}}(v_{n}),\dots,v_{1},\dots v_{n} \}$ （PS：其中 $(T-\lambda I)^{m_i}(v_i)$ 是 eigenvector）是 $G_{\lambda}$ 的一组基，因为对于这组基里面的元素，只要再乘一个 $T-\lambda I$，就变成 0。

从 nullspace 的角度理解 nilpotent。随着 p 变大，$N(T^p)$ 维度逐渐变大，$R(T^p)$ 不断变小，直到 $p=\dim V$ 后维度保持不变。如果 T 是 nilpotent，那么到某个 p，$T^p=0$ 了，$nullity(T^p)=V,rank(T^p)=0$；而如果 T 是 nilpotent，一直到 $p=\dim V$ 了，$nullity(T^p),rank(T^p)$ 都不变并且两者都不是极端值，此时 $V=R(T^p)\oplus N(T^p)$。

从几何上，重数的个数可理解成某个 eigenvalue 方向的个数，而实际特征空间的维度不足这个方向个数，我就可以先取 eigenspace 的基，再通过 $T-\lambda I$ 这个 nilpotent，找到其余的基。$(T-\lambda I)(v_{1})=v_{2}$ 可以理解成，把 v1 伸缩 $\lambda$ 倍，再加上一个 v2 的 shear。

---

# Reference

- [MIT 线性代数](https://www.bilibili.com/video/BV1ix411f7Yp/?spm_id_from=..search-card.all.click)  
- [linear algebra_3Blue1Brown](https://www.3blue1brown.com/topics/linear-algebra)  
- [Linear Algebra Done Right.pdf](https://linear.axler.net/LADR4e.pdf) [原作者Youtube讲解](https://www.youtube.com/watch?v=lkx2BJcnyxk&list=PLGAnmvB9m7zOBVCZBUUmSinFV0wEir2Vw)
- [[MAT2042]]  
- [[线性代数洞见&问题]]  
